You need three things: a model deployed in your Azure OpenAI resource, a local Python env, and a Prompt Flow chat flow. In VS Code, pick your project’s venv as the interpreter and install the bits: pip install -U promptflow promptflow-tools promptflow-azure. Create the Azure OpenAI connection once. The easiest way is a tiny YAML file that says the connection name, your resource endpoint, and the API version. Use this form: name aoai_chat, type azure_open_ai, api_base https://<your-resource>.openai.azure.com, api_version 2024-02-15-preview. In the Prompt Flow panel choose Connections → “Create from YAML,” select that file, and when macOS asks for Keychain access, enter your Mac login password (you can “Always Allow”). If you prefer CLI, put the same fields plus your key into aoai_conn.yaml and run pf connection create file aoai_conn.yaml, then delete the file.

Open your flow in VS Code. Mark question as the chat input and chat_history as chat history; map the LLM node’s output to answer. On the LLM node set the connection to aoai_chat and set deployment_name to the exact Deployment name you see under Azure OpenAI -> Deployments (e.g., gpt-4o or gpt-4-1; it must match exactly). Now run locally with pf flow test flow /absolute/path/to/your/flow-folder or use the Chat button in the Prompt Flow panel.

If you get “EmptyLLMApiMapping,” you installed packages in a different Python—switch VS Code to your venv and pip install promptflow-tools. If you see “Not all required fields filled,” you likely tried to create the connection from the temp YAML—use Connections → Create from YAML on your saved file. If you see DeploymentNotFound (404), the deployment name doesn’t match what’s in Azure or the connection points to the wrong resource—fix the name or endpoint and rerun. That’s it.